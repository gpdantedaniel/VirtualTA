{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lucido Prototype\n",
    "This proof-of-concept runs locally for now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.converters import MarkdownToDocument, PyPDFToDocument, TextFileToDocument, PPTXToDocument\n",
    "from haystack.components.preprocessors import DocumentSplitter, DocumentCleaner\n",
    "from haystack.components.routers import FileTypeRouter\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "# File routing and reading\n",
    "text_file_converter = TextFileToDocument()\n",
    "markdown_converter = MarkdownToDocument()\n",
    "pdf_converter = PyPDFToDocument()\n",
    "powerpoint_converter = PPTXToDocument()\n",
    "file_type_router = FileTypeRouter(mime_types=[\n",
    "    'text/plain', \n",
    "    'application/pdf', \n",
    "    'text/markdown', \n",
    "    'application/vnd.openxmlformats-officedocument.presentationml.presentation'\n",
    "])\n",
    "\n",
    "# Document joining and pre-processing\n",
    "document_joiner = DocumentJoiner()\n",
    "document_cleaner = DocumentCleaner()\n",
    "document_splitter = DocumentSplitter(split_by='word', split_length=150, split_overlap=50)\n",
    "\n",
    "# Document embedding and writing to\n",
    "document_embedder = SentenceTransformersDocumentEmbedder(model='sentence-transformers/all-MiniLM-L6-v2')\n",
    "document_writer = DocumentWriter(document_store=document_store)\n",
    "\n",
    "# Initializing data ingestion pipeline\n",
    "ingestion_pipeline = Pipeline()\n",
    "ingestion_pipeline.add_component(instance=file_type_router, name=\"file_type_router\")\n",
    "ingestion_pipeline.add_component(instance=text_file_converter, name=\"text_file_converter\")\n",
    "ingestion_pipeline.add_component(instance=markdown_converter, name=\"markdown_converter\")\n",
    "ingestion_pipeline.add_component(instance=pdf_converter, name=\"pypdf_converter\")\n",
    "ingestion_pipeline.add_component(instance=powerpoint_converter, name='powerpoint_converter')\n",
    "ingestion_pipeline.add_component(instance=document_joiner, name=\"document_joiner\")\n",
    "ingestion_pipeline.add_component(instance=document_cleaner, name=\"document_cleaner\")\n",
    "ingestion_pipeline.add_component(instance=document_splitter, name=\"document_splitter\")\n",
    "ingestion_pipeline.add_component(instance=document_embedder, name=\"document_embedder\")\n",
    "ingestion_pipeline.add_component(instance=document_writer, name=\"document_writer\")\n",
    "\n",
    "# Connecting components\n",
    "ingestion_pipeline.connect(\"file_type_router.text/plain\", \"text_file_converter.sources\")\n",
    "ingestion_pipeline.connect(\"file_type_router.application/pdf\", \"pypdf_converter.sources\")\n",
    "ingestion_pipeline.connect(\"file_type_router.text/markdown\", \"markdown_converter.sources\")\n",
    "ingestion_pipeline.connect('file_type_router.application/vnd.openxmlformats-officedocument.presentationml.presentation', 'powerpoint_converter.sources')\n",
    "ingestion_pipeline.connect(\"text_file_converter\", \"document_joiner\")\n",
    "ingestion_pipeline.connect(\"pypdf_converter\", \"document_joiner\")\n",
    "ingestion_pipeline.connect(\"markdown_converter\", \"document_joiner\")\n",
    "ingestion_pipeline.connect('powerpoint_converter', 'document_joiner')\n",
    "ingestion_pipeline.connect(\"document_joiner\", \"document_cleaner\")\n",
    "ingestion_pipeline.connect(\"document_cleaner\", \"document_splitter\")\n",
    "ingestion_pipeline.connect(\"document_splitter\", \"document_embedder\")\n",
    "ingestion_pipeline.connect(\"document_embedder\", \"document_writer\")\n",
    "\n",
    "ingestion_pipeline.draw('drawings/ingestion_pipeline.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 134 0 (offset 0)\n",
      "Ignoring wrong pointing object 140 0 (offset 0)\n",
      "Ignoring wrong pointing object 146 0 (offset 0)\n",
      "Ignoring wrong pointing object 809 0 (offset 0)\n",
      "Ignoring wrong pointing object 811 0 (offset 0)\n",
      "Ignoring wrong pointing object 820 0 (offset 0)\n",
      "Ignoring wrong pointing object 825 0 (offset 0)\n",
      "Ignoring wrong pointing object 925 0 (offset 0)\n",
      "Ignoring wrong pointing object 931 0 (offset 0)\n",
      "Ignoring wrong pointing object 937 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 263 0 (offset 0)\n",
      "Ignoring wrong pointing object 269 0 (offset 0)\n",
      "Ignoring wrong pointing object 319 0 (offset 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df7b5798355942349ba2c63372770051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'document_writer': {'documents_written': 266}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Testing the ingestion pipeline\n",
    "content_dir = 'bien210'\n",
    "ingestion_pipeline.run({'file_type_router': {'sources': list(Path(content_dir).glob(\"**/*\"))}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RAG Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Augmenting ambiguous queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from haystack.components.generators.openai import OpenAIGenerator\n",
    "from haystack.components.generators.hugging_face_api import HuggingFaceAPIGenerator\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack import component\n",
    "\n",
    "disambiguator_llm = HuggingFaceAPIGenerator(\n",
    "  api_type=\"serverless_inference_api\",\n",
    "  api_params={\"model\": \"microsoft/Phi-3.5-mini-instruct\"},\n",
    "  generation_kwargs={\n",
    "    'max_new_tokens': 25,\n",
    "    'temperature': 100,\n",
    "    'top_k': 1,\n",
    "    'top_p': 0.1\n",
    "  }\n",
    ")\n",
    "\n",
    "@component\n",
    "class QueryDisambiguator:\n",
    "  \"\"\"\n",
    "  A component that disambiguates the query using the conversation history.\n",
    "  \"\"\"\n",
    "  @component.output_types(disambiguated_query=str)\n",
    "  def run(self, query: str, messages: List[ChatMessage]):\n",
    "    chat = \" \".join(list(map(lambda m: m.content, messages)))\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful AI assistant. You identify and name the topic that the current query refers to.\n",
    "    Based on the context, name the topic that the current query is referring to. Be brief\n",
    "    \n",
    "    Context:\n",
    "    {chat}\n",
    "\n",
    "    Current Query:\n",
    "    {query}\n",
    "\n",
    "    Topic:\n",
    "    \"\"\"\n",
    "    result = disambiguator_llm.run(prompt=prompt)\n",
    "    disambiguated_query = result['replies'][0]\n",
    "    print('disambiguated_query: ', disambiguated_query)\n",
    "\n",
    "    return {'disambiguated_query': disambiguated_query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disambiguated_query:   Limitations of Lateral Flow Assays\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'disambiguated_query': ' Limitations of Lateral Flow Assays\\n\\n'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_messages = [\n",
    "  ChatMessage.from_system(\"You are Lucido, a helpful teaching assistant. Answer questions based on hints provided to you by the system. If the hints are insufficient, inform the user that you do not have the knowledge to answer that question.\"),\n",
    "  ChatMessage.from_user('How do you build a lateral flow assay, and more specifically, what are its components?'),\n",
    "  ChatMessage.from_assistant(\"\"\"To build a lateral flow assay, you need to assemble the following components:\n",
    "  Sample Pad: This pad is typically made of chromatographic materials such as cellulose or glass fiber. It enables the flow of the sample towards the other parts of the assay by capillary action. It also helps in removing unwanted substances from the sample, such as salt or other impurities that may interfere with the assay's performance.\n",
    "  Conjugate Pad: This pad is placed immediately after the sample pad, and it holds the conjugate solution, which is a mixture of a labeling molecule (such as gold nanoparticles or fluorescent dye) and the antibody which will bind to the analyte. Capillary action then draws the conjugate solution onto the test area through the sample pad.\n",
    "  Test Line: This line contains the second antibody immobilized on the membrane. This antibody will bind only to the analyte-conjugate complex, specifically preventing non-specific binding. The secondary antibody is labeled with a colored molecule, such as gold nanoparticles or a red dye. This line will turn pink or red when the analyte is present, as these labeled molecules show up under a magnifying glass or a microscope.\n",
    "  Control Line: This line is comprised of a single antibody that binds to the initial antibody in the conjugate pad, not the analyte. This line ensures that the entire assay still works for control purposes, as the labeled molecules will still get released in the absence of the analyte.\n",
    "  Moisture Absorption Pad: Also known as the absorbent pad, it is placed at the end of the membrane to absorb any excess solution, specifically, the conjugate or analyte solutions, to prevent false results.\n",
    "  The lateral flow assay works by drawing the sample through the membrane, and the antibody-analyte bond causes a visible line at the test line. A control line confirms that the test is working correctly. Compared to other tests that require the use of a reader or other instruments, lateral flow assays are more straightforward and can provide instant results, making them suitable for use in developing countries and in resource-poor settings where laboratory equipment is not easily available.\n",
    "  \"\"\")\n",
    "]\n",
    "\n",
    "query = 'What are its limitations?'\n",
    "disambiguator = QueryDisambiguator()\n",
    "disambiguator.run(query=query, messages=example_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Building the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever, InMemoryBM25Retriever\n",
    "from haystack.components.rankers import TransformersSimilarityRanker\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators.hugging_face_api import HuggingFaceAPIGenerator\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the given context. Be brief.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{ question }}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Initialize disambiguator the embedder \n",
    "disambiguator = QueryDisambiguator()\n",
    "text_embedder = SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize store retrievers\n",
    "embedding_retriever = InMemoryEmbeddingRetriever(document_store=document_store)\n",
    "bm25_retriever = InMemoryBM25Retriever(document_store=document_store)\n",
    "\n",
    "# Initialize document joiner and ranker\n",
    "document_joiner = DocumentJoiner()\n",
    "ranker = TransformersSimilarityRanker(model='BAAI/bge-reranker-base')\n",
    "\n",
    "# Initialize LLM generation components\n",
    "prompt_builder = PromptBuilder(template=template)\n",
    "llm = HuggingFaceAPIGenerator(\n",
    "  api_type=\"serverless_inference_api\",\n",
    "  api_params={\"model\": \"HuggingFaceH4/zephyr-7b-beta\"},\n",
    "  generation_kwargs={'max_new_tokens': 150}\n",
    ")\n",
    "\n",
    "# Initialize the RAG pipeline\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component('disambiguator', disambiguator)\n",
    "rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "rag_pipeline.add_component(\"embedding_retriever\", embedding_retriever)\n",
    "rag_pipeline.add_component('bm25_retriever', bm25_retriever)\n",
    "rag_pipeline.add_component('document_joiner', document_joiner)\n",
    "rag_pipeline.add_component('ranker', ranker)\n",
    "rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "rag_pipeline.add_component('llm', llm)\n",
    "\n",
    "# Connecting the components\n",
    "rag_pipeline.connect('disambiguator.disambiguated_query', 'text_embedder.text')\n",
    "rag_pipeline.connect('disambiguator.disambiguated_query', 'bm25_retriever.query')\n",
    "rag_pipeline.connect('disambiguator.disambiguated_query', 'ranker.query')\n",
    "rag_pipeline.connect('disambiguator.disambiguated_query', 'prompt_builder.question')\n",
    "rag_pipeline.connect('text_embedder', 'embedding_retriever')\n",
    "rag_pipeline.connect('embedding_retriever', 'document_joiner')\n",
    "rag_pipeline.connect('bm25_retriever', 'document_joiner')\n",
    "rag_pipeline.connect('document_joiner', 'ranker')\n",
    "rag_pipeline.connect(\"ranker.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect('prompt_builder', 'llm')\n",
    "\n",
    "rag_pipeline.draw('drawings/rag_pipeline.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradio Chatbot Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.generators.chat import HuggingFaceAPIChatGenerator\n",
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "if \"HF_API_TOKEN\" not in os.environ:\n",
    "    os.environ[\"HF_API_TOKEN\"] = getpass(\"Enter HuggingFace API token:\")\n",
    "\n",
    "messages = [ChatMessage.from_system(\"\"\"\n",
    "You are Lucido, a helpful teaching assistant. Answer questions based on hints provided to you by the system. \n",
    "If the hints are insufficient, inform the user that you do not have the knowledge to answer that question.\n",
    "\"\"\")]\n",
    "\n",
    "chat_generator = HuggingFaceAPIChatGenerator(\n",
    "  api_type=\"serverless_inference_api\",\n",
    "  api_params={\"model\": \"HuggingFaceH4/zephyr-7b-beta\"},\n",
    "  # generation_kwargs={'max_tokens': 150}\n",
    ")\n",
    "\n",
    "def chatbot(query, _):\n",
    "  # Generate the RAG response \n",
    "  rag_response = rag_pipeline.run({\n",
    "    'disambiguator': {\n",
    "       'query': query, \n",
    "       'messages': messages\n",
    "      },\n",
    "  })\n",
    "  \n",
    "  rag_reply = rag_response['llm']['replies'][0]\n",
    "  messages.append(ChatMessage.from_system(rag_reply))\n",
    "  messages.append(ChatMessage.from_user(query))\n",
    "  result = chat_generator.run(messages=messages)\n",
    "  reply = result['replies'][0].content\n",
    "\n",
    "  return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\VirtualTA\\venv\\Lib\\site-packages\\gradio\\components\\chatbot.py:248: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\VirtualTA\\venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2245: FutureWarning: `stop_sequences` is a deprecated argument for `text_generation` task and will be removed in version '0.28.0'. Use `stop` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disambiguated_query:  1. How could artificial leaves benefit society?\n",
      "    2. How do artificial leaves compare to natural photosynthesis?\n",
      "\n",
      "Support: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83dae08b7e884f41a828b3ce3a00125b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disambiguated_query:  \n",
      "Why can the cell membrane be compared to a PN junction in terms of their functions or properties?\n",
      "\n",
      "\n",
      "## Response: How\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d1a2e2467147a99a6cfef9a8f8641e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def upload_file(filepath):\n",
    "  print(filepath)\n",
    "  ingestion_pipeline.run({'file_type_router': {'sources': [Path(filepath)]}})\n",
    "\n",
    "with gr.Blocks() as demo: \n",
    "  gr.ChatInterface(\n",
    "    fn=chatbot, \n",
    "    title=\"Lucido — A BIEN 210 Application\",\n",
    "    examples=[\n",
    "      'How does myelination of the axons help increase signal speeds in neurons?',\n",
    "      'How does bioluminescence differ from chemiluminescence?',\n",
    "      'Why can the cell membrane be compared to a PN-junction?',\n",
    "      'How could artificial leaves help society and how do they compare to photosynthesis?',\n",
    "      'What are the two main superhydrophobic regimes and how can they be applied?',\n",
    "      'How do you build a lateral flow assay and, more specifically, what are its components?'\n",
    "    ]\n",
    "  )\n",
    "  \n",
    "  upload_button = gr.UploadButton('Upload a file', file_count='single', variant='primary')\n",
    "  upload_button.upload(upload_file, upload_button)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
