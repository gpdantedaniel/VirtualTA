{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lucido Prototype\n",
    "This proof-of-concept runs locally for now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.converters import MarkdownToDocument, PyPDFToDocument, TextFileToDocument, PPTXToDocument\n",
    "from haystack.components.preprocessors import DocumentSplitter, DocumentCleaner\n",
    "from haystack.components.routers import FileTypeRouter\n",
    "from haystack.components.joiners import DocumentJoiner\n",
    "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "\n",
    "# File routing and reading\n",
    "file_type_router = FileTypeRouter(mime_types=['text/plain', 'application/pdf', 'text/markdown', 'application/vnd.openxmlformats-officedocument.presentationml.presentation'])\n",
    "text_file_converter = TextFileToDocument()\n",
    "markdown_converter = MarkdownToDocument()\n",
    "pdf_converter = PyPDFToDocument()\n",
    "powerpoint_converter = PPTXToDocument()\n",
    "\n",
    "# Document joining and pre-processing\n",
    "document_joiner = DocumentJoiner()\n",
    "document_cleaner = DocumentCleaner()\n",
    "document_splitter = DocumentSplitter(split_by='word', split_length=150, split_overlap=50)\n",
    "\n",
    "# Document embedding and writing to\n",
    "document_embedder = SentenceTransformersDocumentEmbedder(model='sentence-transformers/all-MiniLM-L6-v2')\n",
    "document_writer = DocumentWriter(document_store=document_store)\n",
    "\n",
    "# Initializing data ingestion pipeline\n",
    "ingestion_pipeline = Pipeline()\n",
    "ingestion_pipeline.add_component(instance=file_type_router, name=\"file_type_router\")\n",
    "ingestion_pipeline.add_component(instance=text_file_converter, name=\"text_file_converter\")\n",
    "ingestion_pipeline.add_component(instance=markdown_converter, name=\"markdown_converter\")\n",
    "ingestion_pipeline.add_component(instance=pdf_converter, name=\"pypdf_converter\")\n",
    "ingestion_pipeline.add_component(instance=powerpoint_converter, name='powerpoint_converter')\n",
    "ingestion_pipeline.add_component(instance=document_joiner, name=\"document_joiner\")\n",
    "ingestion_pipeline.add_component(instance=document_cleaner, name=\"document_cleaner\")\n",
    "ingestion_pipeline.add_component(instance=document_splitter, name=\"document_splitter\")\n",
    "ingestion_pipeline.add_component(instance=document_embedder, name=\"document_embedder\")\n",
    "ingestion_pipeline.add_component(instance=document_writer, name=\"document_writer\")\n",
    "\n",
    "# Connecting components\n",
    "ingestion_pipeline.connect(\"file_type_router.text/plain\", \"text_file_converter.sources\")\n",
    "ingestion_pipeline.connect(\"file_type_router.application/pdf\", \"pypdf_converter.sources\")\n",
    "ingestion_pipeline.connect(\"file_type_router.text/markdown\", \"markdown_converter.sources\")\n",
    "ingestion_pipeline.connect('file_type_router.application/vnd.openxmlformats-officedocument.presentationml.presentation', 'powerpoint_converter.sources')\n",
    "ingestion_pipeline.connect(\"text_file_converter\", \"document_joiner\")\n",
    "ingestion_pipeline.connect(\"pypdf_converter\", \"document_joiner\")\n",
    "ingestion_pipeline.connect(\"markdown_converter\", \"document_joiner\")\n",
    "ingestion_pipeline.connect('powerpoint_converter', 'document_joiner')\n",
    "ingestion_pipeline.connect(\"document_joiner\", \"document_cleaner\")\n",
    "ingestion_pipeline.connect(\"document_cleaner\", \"document_splitter\")\n",
    "ingestion_pipeline.connect(\"document_splitter\", \"document_embedder\")\n",
    "ingestion_pipeline.connect(\"document_embedder\", \"document_writer\")\n",
    "\n",
    "ingestion_pipeline.draw('drawings/ingestion_pipeline.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 134 0 (offset 0)\n",
      "Ignoring wrong pointing object 140 0 (offset 0)\n",
      "Ignoring wrong pointing object 146 0 (offset 0)\n",
      "Ignoring wrong pointing object 809 0 (offset 0)\n",
      "Ignoring wrong pointing object 811 0 (offset 0)\n",
      "Ignoring wrong pointing object 820 0 (offset 0)\n",
      "Ignoring wrong pointing object 825 0 (offset 0)\n",
      "Ignoring wrong pointing object 925 0 (offset 0)\n",
      "Ignoring wrong pointing object 931 0 (offset 0)\n",
      "Ignoring wrong pointing object 937 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 263 0 (offset 0)\n",
      "Ignoring wrong pointing object 269 0 (offset 0)\n",
      "Ignoring wrong pointing object 319 0 (offset 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3345aea11ed241f3ad6f53f9fca86ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'document_writer': {'documents_written': 42}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Testing the ingestion pipeline\n",
    "# content_dir = 'bien210'\n",
    "# ingestion_pipeline.run({'file_type_router': {'sources': list(Path(content_dir).glob(\"**/*\"))}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RAG Chat Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.builders import ChatPromptBuilder\n",
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "template = [ChatMessage.from_system(\"\"\"\n",
    "Answer the questions based on the given context.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "Question: {{ question }}\n",
    "Answer:\n",
    "\"\"\")]\n",
    "\n",
    "# Initializing the RAG pipeline\n",
    "rag_pipeline = Pipeline()\n",
    "rag_pipeline.add_component(\"embedder\", SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-MiniLM-L6-v2\"))\n",
    "rag_pipeline.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store, top_k=5))\n",
    "rag_pipeline.add_component(\"prompt_builder\", ChatPromptBuilder(template=template))\n",
    "\n",
    "# Connecting the components\n",
    "rag_pipeline.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "\n",
    "rag_pipeline.draw('drawings/rag_pipeline.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a41c9be79244debf9058d5080e6832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nAnswer the questions based on the given context.\\n\\nContext:\\n\\n    E-SKIN AND AN APPLICATION in\\x0bROBOTIC SURGERY\\n(Sarwar, 2023)\\x0cINTRODUCTION\\x0cWHAT IS E-SKIN?\\nE-skin aims to mimic human skin\\nMechanical properties: Adherence to irregular surfaces, stretchability, mechanical toughness, and self-healing properties Tactile sensing: Pressure, strain, temperature, humidity, shear force\\nInsights: Shapes of objects, surfaces, texture, hardness, etc.\\nApplications are found in skin-attachable electronics for healthcare, robotics and prosthetics (Yang et al., 2019)\\n(Chen et al., 2021; Yang et al., 2019).\\x0cTELEOPERATED SURGERY\\nRobots used to perform surgery since the 1980s\\nStatic positioning of tools, automated trajectories in preoperative planning, teleoperation and rehabilitation\\nMaster-slave systems consist of a robotic manipulator and a controller (e.g., Da Vinci)\\nAccurate positioning, repeatability, better posture for the surgeon, teleperesence, minimally invasive procedures\\n(Schäfer et al., 2019; El Rassi et al., 2020).\\x0cLiterature review and current applications\\x0cLiterature\\nSensors are either tactile or chemical in the kind of stimuli they pick up. One same stimulus can be observed using different technologies (Chen et al., 2021).\\nTactile e-skin sensors and technologies (Chen et al., 2021). :\\nStrain: \\n\\n    minimally invasive procedures\\n(Schäfer et al., 2019; El Rassi et al., 2020).\\x0cLiterature review and current applications\\x0cLiterature\\nSensors are either tactile or chemical in the kind of stimuli they pick up. One same stimulus can be observed using different technologies (Chen et al., 2021).\\nTactile e-skin sensors and technologies (Chen et al., 2021). :\\nStrain: resistive sensors based on conductive fillers Pressure: piezoresistive materials and capacitators\\nShear force: interlocked structures and varying resistance\\nTemperature: thermistors and ionic hydrogels (Chen et al., 2021).\\x0cLiterature\\nMaterials that make up e-skin must provide flexibility, stretchability, sensitivity, self-healing and biocompatibility (Chen et al., 2021).\\nCommon materials (Chen et al., 2021). :\\nElastomers: PDMS, PU and SEBS Stretchable conductors: CNTS and silver nanowires\\nStretchable semiconductors: organic polymers (e.g., polythiophene).\\nHydrogels and ionogels: high ionic conductivity that varies with strain and temperature\\nSelf-healing materials: Metal-ligand polymers (Chen et al., 2021).\\x0cLiterature\\nHow do you embed sensors on the skin membrane?We can engineer stretchability using:\\nGeometric Engineering\\nStructural designs distribute strain across brittle materials (serpentine, \\n\\n    any disagreement between the Majik system and the flex sensors the robotic hand will freeze and enter a \"shutdown mode\"\\nShut down switches\\x0cEffectiveness of haptic feedback Bergholz et al., 2023\\nDecreases forces applied by robotic hand (Hedge\\'s g = 0.83)\\nReduces completion time (Hedges’ g\\u2009=\\u20090.83)\\nHigher accuracy (Hedges’ g\\u2009=\\u20091.50)\\nHigher success rates (Hedges’ g\\u2009=\\u20090.80)\\x0cImpacts and suggestions\\x0cImpacts\\nSophisticated e-skin\\nSurgical training\\nRobot learning SOCIETY\\nImprove healthcare in remote regions\\nFaster response time in emergency Could help save more lives MEDICAL FIELD\\nEnhance precision of procedures\\nShare knowledge and expertise NEXT steps…\\n(Duncan, 2024)\\x0cCONCLUSION\\nE-Skin is versatile\\n(Suh, 2024)\\x0cReferences\\nAdvanced Materials - 2019 - Yang - Electronic Skin Recent Progress and Future Prospects for Skin‐Attachable Devices.\\nChen, J., Zhu, Y., Chang, X., Pan, D., Song, G., Guo, Z., & Naik, N. (2021). Recent progress in essential functions of soft electronic skin.\\xa0Advanced Functional Materials,\\xa031(42), 2104686.\\nChortos, A., Liu, J., & Bao, Z. (2016). Pursuing prosthetic electronic skin.\\xa0Nature materials,\\xa015(9), 937-950.\\nDeng, M., Fan, F., & Wei, X. (2023). Learning-based object recognition via a \\n\\n    Y., Chang, X., Pan, D., Song, G., Guo, Z., & Naik, N. (2021). Recent progress in essential functions of soft electronic skin.\\xa0Advanced Functional Materials,\\xa031(42), 2104686.\\nChortos, A., Liu, J., & Bao, Z. (2016). Pursuing prosthetic electronic skin.\\xa0Nature materials,\\xa015(9), 937-950.\\nDeng, M., Fan, F., & Wei, X. (2023). Learning-based object recognition via a EUTECTOGEL Electronic Skin Enabled Soft Robotic gripper. IEEE Robotics and Automation Letters, 8(11), 7424–7431. https://doi.org/10.1109/lra.2023.3316096 Duncan, G. (2024). Optimizing a holistic training journey for surgeons. https://www.intuitive.com/en-us/about-us/newsroom/optimizing-surgeon-training\\nEditors of The Swiss Medical Network (2024). Da Vinci. Retrieved 2024-11-11, from https://www.swissmedical.net/en/robots/da-vinci\\nEl Rassi, I., & El Rassi, J. M. (2020). A review of haptic feedback in tele-operated robotic surgery. Journal of medical engineering & technology, 44(5), 247-254.\\nKim, U., Jung, D., Jeong, H.\\xa0et al.\\xa0Integrated linkage-driven dexterous anthropomorphic robotic hand.\\xa0Nat Commun\\xa012, 7177 (2021). https://doi.org/10.1038/s41467-021-27261-0\\nLi, D., Zhou, J., Yao, K., Liu, S., He, J., Su, J., Qu, Q., Gao, Y., Song, Z., Yiu, C., Sha, C., Sun, \\n\\n    on a Kinematic Model of the Hand. IEEE Transactions on Instrumentation and Measurement, 70, 1-13. https://doi.org/10.1109/TIM.2021.3065761 Sarwar, N. (2023). This incredible \\'e-skin\\' technology communicates with the brain like real skin. https://www.slashgear.com/1299510/incredible-e-skin-technology/\\nSchäfer, M. B., Stewart, K. W., & Pott, P. P. (2019). Industrial robots for teleoperated surgery–a systematic review of existing approaches.\\xa0Current Directions in Biomedical Engineering,\\xa05(1), 153-156.\\nSonar, H., Huang, J.-L., & Paik, J. (2021). Soft Touch using Soft Pneumatic Actuator–Skin as a Wearable Haptic Feedback Device. Advanced Intelligent Systems, 3, 2000168. https://doi.org/10.1002/aisy.202000168 SpectraSymbol. (2021). Our resistive flex sensors. https://www.spectrasymbol.com/resistive-flex-sensors\\nSuh, C. (2024). How flexible electronic skin is created. Britannica. https://www.britannica.com/video/Flexible-electronic-skin-help-humans-and-machines-interact/-246912\\nThomas, E. (2023). 5G remote-controlled robots will enable surgery from afar. https://www.medicaldevice-network.com/news/5g-remote-controlled-robots-will-enable-surgery-from-afar/\\nYang, J. C., Mun, J., Kwon, S. Y., Park, S., Bao, Z., & Park, S. (2019). Electronic skin: recent progress and future prospects for skin‐attachable devices for health monitoring, robotics, and prosthetics.\\xa0Advanced Materials,\\xa031(48), 1904765.\\nYu, M., Cheng, X., Peng, S., Cao, Y., Lu, \\n\\nQuestion: What is e-skin?\\nAnswer:'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the rag chat pipeline\n",
    "query = 'What is e-skin?'\n",
    "result = rag_pipeline.run({\n",
    "  'embedder': {'text': query}, \n",
    "  'prompt_builder': {'question': query}\n",
    "})\n",
    "\n",
    "result['prompt_builder']['prompt'][0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradio Chatbot Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OpenAI API token:\")\n",
    "\n",
    "messages = [ChatMessage.from_system(\"You are a virtual teaching assistant. Answer questions based on the given context\")]\n",
    "chat_generator = OpenAIChatGenerator(model='gpt-3.5-turbo')\n",
    "\n",
    "def chatbot(query:str, history) -> str:\n",
    "  # Generate the RAG prompt \n",
    "  rag_response = rag_pipeline.run({'embedder': {'text': query}, 'prompt_builder': {'question': query}})\n",
    "  rag_prompt = rag_response['prompt_builder']['prompt'][0].content\n",
    "  messages.append(ChatMessage.from_user(rag_prompt))\n",
    "\n",
    "  # Generate the LLM response\n",
    "  llm_response = chat_generator.run(messages=messages)\n",
    "  reply = llm_response['replies'][0]\n",
    "  messages.append(reply)\n",
    "\n",
    "  return reply.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the chatbot function\n",
    "# chatbot(\"What is electric skin?\", _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\VirtualTA\\venv\\Lib\\site-packages\\gradio\\components\\chatbot.py:248: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ddede245454c51a8c2958f392c19d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7630bbf4d00142919156dbf472bcd3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c0a0b4c70c496ca682fc9b00367609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f6ae8b6f9844aca8738b5567058c85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3cca7d97c545859734cf3ac3ae06e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d13a9a50ff46b4bd09cbf05b4fa892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3fe957dfc64ba0a5baef639c154cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a364d7ae770f47d29edb4ed37d4e08d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo = gr.ChatInterface(fn=chatbot, title=\"Lucido — A BIEN 210 Application\",)\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
